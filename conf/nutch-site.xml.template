<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>
<property>
  <name>http.agent.name</name>
  <value>Bot</value>
  <description>HTTP 'User-Agent' request header. MUST NOT be empty -
  please set this to a single word uniquely related to your organization.

  NOTE: You should also check other related properties:

    http.robots.agents
    http.agent.description
    http.agent.url
    http.agent.email
    http.agent.version

  and set their values appropriately.

  </description>
</property>

<property>
  <name>http.robot.rules.whitelist</name>
  <value>faz.net</value>
  <description>Comma separated list of hostnames or IP addresses to ignore
  robot rules parsing for. Use with care and only if you are explicitly
  allowed by the site owner to ignore the site's robots.txt!
  </description>
</property>



<property>
 <name>http.agent.description</name>
 <value>bot description</value>
 <description>Further description of our bot- this text is used in
 the User-Agent header.  It appears in parenthesis after the agent name.
 </description>
</property>

<property>
  <name>http.agent.version</name>
  <value>Crawl-1.13</value>
  <description>A version string to advertise in the User-Agent
   header.</description>
</property>

<property>
  <name>http.redirect.max</name>
  <value>3</value>
  <description>The maximum number of redirects the fetcher will follow when
  trying to fetch a page. If set to negative or 0, fetcher won't immediately
  follow redirected URLs, instead it will record them for later fetching.
  </description>
</property>


<property>
  <name>http.useHttp11</name>
  <value>false</value>
  <description>NOTE: at the moment this works only for protocol-httpclient.
  If true, use HTTP 1.1, if false use HTTP 1.0 .
  </description>
</property>


<property>
  <name>http.accept.language</name>
  <value>de-de,en-us,en;q=0.7,*;q=0.3</value>
  <description>Value of the "Accept-Language" request header field.
  This allows selecting non-English language as default one to retrieve.
  It is a useful setting for search engines build for certain national group.
  </description>
</property>


<property>
  <name>http.accept</name>
  <value>text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8</value>
  <description>Value of the "Accept" request header field.
  </description>
</property>

<property>
  <name>scoring.depth.max</name>
  <value>3</value>
  <description>Max depth value from seed allowed by default.
  Can be overridden on a per-seed basis by specifying "_maxdepth_=VALUE"
  as a seed metadata. This plugin adds a "_depth_" metadatum to the pages
  to track the distance from the seed it was found from.
  The depth is used to prioritise URLs in the generation step so that
  shallower pages are fetched first.
  </description>
</property>

<!-- disable refetching of items as the fetch date is needed to determine the first occurence -->

  <property>
    <name>db.fetch.interval.default</name>
    <value>315360000</value>
    <description>The default number of seconds between re-fetches of a page (10 years).
    </description>
  </property>

  <property>
    <name>db.fetch.interval.max</name>
    <value>315360000</value>
    <description>The maximum number of seconds between re-fetches of a page
      (10 years). After this period every page in the db will be re-tried, no
      matter what is its status.
    </description>
  </property>



<property>
  <name>db.fetch.schedule.class</name>
  <value>org.apache.nutch.crawl.DefaultFetchSchedule</value>
  <description>The implementation of fetch schedule. DefaultFetchSchedule simply
  adds the original fetchInterval to the last fetch time, regardless of
  page changes, whereas AdaptiveFetchSchedule (see below) tries to adapt
  to the rate at which a given page is changed.
  </description>
</property>

<property>
  <name>db.update.max.inlinks</name>
  <value>100000</value>
  <description>Maximum number of inlinks to take into account when updating
  a URL score in the crawlDB. Only the best scoring inlinks are kept.
  </description>
</property>

<property>
  <name>db.ignore.internal.links</name>
  <value>false</value>
  <description>If true, outlinks leading from a page to internal hosts or domain
  will be ignored. This is an effective way to limit the crawl to include
  only initially injected hosts, without creating complex URLFilters.
  See 'db.ignore.external.links.mode'.
  </description>
</property>

<property>
  <name>db.ignore.external.links</name>
  <value>true</value>
  <description>If true, outlinks leading from a page to external hosts or domain
  will be ignored. This is an effective way to limit the crawl to include
  only initially injected hosts, without creating complex URLFilters.
  See 'db.ignore.external.links.mode'.
  </description>
</property>


<property>
  <name>db.ignore.external.links.mode</name>
  <value>byDomain</value>
  <description>Alternative value is byDomain</description>
</property>

 <property>
  <name>db.injector.overwrite</name>
  <value>true</value>
  <description>Whether existing records in the CrawlDB will be overwritten
  by injected records.
  </description>
</property>


<property>
  <name>db.max.outlinks.per.page</name>
  <value>-1</value>
  <description>The maximum number of outlinks that we'll process for a page.
  If this value is nonnegative (>=0), at most db.max.outlinks.per.page outlinks
  will be processed for a page; otherwise, all outlinks will be processed.
  </description>
</property>


<property>
  <name>db.max.anchor.length</name>
  <value>1000</value>
  <description>The maximum number of characters permitted in an anchor.
  </description>
</property>

<property>
  <name>linkdb.max.inlinks</name>
  <value>100000</value>
  <description>Maximum number of Inlinks per URL to be kept in LinkDb.
  If "invertlinks" finds more inlinks than this number, only the first
  N inlinks will be stored, and the rest will be discarded.
  </description>
</property>


<property>
  <name>linkdb.ignore.internal.links</name>
  <value>false</value>
  <description>If true, when adding new links to a page, links from
  the same host are ignored.  This is an effective way to limit the
  size of the link database, keeping only the highest quality
  links.
  </description>
</property>


<property>
  <name>partition.url.mode</name>
  <value>byDomain</value>
  <description>Determines how to partition URLs. Default value is 'byHost',
  also takes 'byDomain' or 'byIP'.
  </description>
</property>


<property>
  <name>fetcher.server.delay</name>
  <value>2.0</value>
  <description>The number of seconds the fetcher will delay between
   successive requests to the same server. Note that this might get
   overridden by a Crawl-Delay from a robots.txt and is used ONLY if
   fetcher.threads.per.queue is set to 1.
   </description>
</property>

<property>
  <name>fetcher.threads.fetch</name>
  <value>10</value>
  <description>The number of FetcherThreads the fetcher should use.
  This is also determines the maximum number of requests that are
  made at once (each FetcherThread handles one connection). The total
  number of threads running in distributed mode will be the number of
  fetcher threads * number of nodes as fetcher has one map task per node.
  </description>
</property>

<property>
  <name>fetcher.threads.per.queue</name>
  <value>5</value>
  <description>This number is the maximum number of threads that
    should be allowed to access a queue at one time. Setting it to
    a value > 1 will cause the Crawl-Delay value from robots.txt to
    be ignored and the value of fetcher.server.min.delay to be used
    as a delay between successive requests to the same server instead
    of fetcher.server.delay.
   </description>
</property>


<property>
  <name>fetcher.queue.mode</name>
  <value>byDomain</value>
  <description>Determines how to put URLs into queues. Default value is 'byHost',
  also takes 'byDomain' or 'byIP'.
  </description>
</property>



<property>
  <name>fetcher.throughput.threshold.pages</name>
  <value>-1</value>
  <description>The threshold of minimum pages per second. If the fetcher downloads less
  pages per second than the configured threshold, the fetcher stops, preventing slow queue's
  from stalling the throughput. This threshold must be an integer. This can be useful when
  fetcher.timelimit.mins is hard to determine. The default value of -1 disables this check.
  </description>
</property>

<property>
  <name>fetcher.throughput.threshold.retries</name>
  <value>5</value>
  <description>The number of times the fetcher.throughput.threshold is allowed to be exceeded.
  This settings prevents accidental slow downs from immediately killing the fetcher thread.
  </description>
</property>

<property>
  <name>fetcher.throughput.threshold.check.after</name>
  <value>5</value>
  <description>The number of minutes after which the throughput check is enabled.</description>
</property>



<property>
  <name>fetcher.queue.depth.multiplier</name>
  <value>200</value>
  <description>(EXPERT)The fetcher buffers the incoming URLs into queues based on the [host|dom
ain|IP]
  (see param fetcher.queue.mode). The depth of the queue is the number of threads times the val
ue of this parameter.
  A large value requires more memory but can improve the performance of the fetch when the orde
r of the URLS in the fetch list
  is not optimal.
  </description>
</property>



<property>
  <name>fetcher.queue.depth.multiplier</name>
  <value>50</value>
  <description>(EXPERT)The fetcher buffers the incoming URLs into queues based on the [host|dom
ain|IP]
  (see param fetcher.queue.mode). The depth of the queue is the number of threads times the val
ue of this parameter.
  A large value requires more memory but can improve the performance of the fetch when the orde
r of the URLS in the fetch list
  is not optimal.
  </description>
</property>


<!-- linkrank scoring properties -->

<property>
  <name>link.ignore.internal.host</name>
  <value>false</value>
  <description>Ignore outlinks to the same hostname.</description>
</property>

<property>
  <name>link.ignore.internal.domain</name>
  <value>false</value>
  <description>Ignore outlinks to the same domain.</description>
</property>


<property>
  <name>link.ignore.limit.domain</name>
  <value>false</value>
  <description>Limit to only a single outlink to the same domain.</description>
</property>

<property>
  <name>http.proxy.host</name>
  <value>10.1.1.20</value>
  <description>The proxy hostname.  If empty, no proxy is used.</description>
</property>

<property>
  <name>http.proxy.port</name>
  <value>8000</value>
  <description>The proxy port.</description>
</property>
<!--
<property>
  <name>http.proxy.username</name>
  <value></value>
  <description>Username for proxy. This will be used by
  'protocol-httpclient', if the proxy server requests basic, digest
  and/or NTLM authentication. To use this, 'protocol-httpclient' must
  be present in the value of 'plugin.includes' property.
  NOTE: For NTLM authentication, do not prefix the username with the
  domain, i.e. 'susam' is correct whereas 'DOMAIN\susam' is incorrect.
  </description>
</property>

<property>
  <name>http.proxy.password</name>
  <value></value>
  <description>Password for proxy. This will be used by
  'protocol-httpclient', if the proxy server requests basic, digest
  and/or NTLM authentication. To use this, 'protocol-httpclient' must
  be present in the value of 'plugin.includes' property.
  </description>
</property>
-->

<!-- elasticsearch indexer -->

 <property>
   <name>elastic.host</name>
   <value>localhost</value>
   <description>The hostname to send documents to using TransportClient. Either host
     and port must be defined or cluster.</description>
 </property>

 <property>
   <name>elastic.port</name>
   <value>9300</value>
   <description>The port to connect to using TransportClient.</description>
 </property>

 <property>
   <name>elastic.cluster</name>
   <value></value>
   <description>The cluster name to discover. Either host and port must be defined
     or cluster.</description>
 </property>

 <property>
   <name>elastic.index</name>
   <value>nutch</value>
   <description>Default index to send documents to.</description>
 </property>

 <property>
   <name>elastic.max.bulk.docs</name>
   <value>250</value>
   <description>Maximum size of the bulk in number of documents.</description>
 </property>

 <property>
   <name>elastic.max.bulk.size</name>
   <value>2500500</value>
   <description>Maximum size of the bulk in bytes.</description>
 </property>

<!-- /elasticsearch indexer -->

<property>
  <name>plugin.includes</name>
  <value>protocol-httpclient|urlfilter-regex|extractor|parse-(html|tika)|index-(basic|anchh
or)|indexer-elastic|scoring-depth|urlnormalizer-(pass|regex|basic)</value>
  <description>Regular expression naming plugin directory names to
  include.  Any plugin not matching this expression is excluded.
  In any case you need at least include the nutch-extensionpoints plugin. By
  default Nutch includes crawling just HTML and plain text via HTTP,
  and basic indexing and search plugins. In order to use HTTPS please enable
  protocol-httpclient, but be aware of possible intermittent problems with the
  underlying commons-httpclient library. Set parsefilter-naivebayes for classifii
cation based focused crawler.
  </description>
</property>
<!-- Extractor Config -->
<property>
  <name>extractor.file</name>
  <value>extractors.xml</value>
</property>
</configuration>

